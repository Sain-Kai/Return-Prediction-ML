{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b730857f-36a1-4f61-b6b7-14159fcc1dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6555dd00-8869-4841-b348-ed3ea1f447c1",
   "metadata": {},
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9c3e91f-d689-475b-a9dc-3a67eaf01c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.read_excel(\"Retail-Supply-Chain-Sales-Dataset.xlsx\")\n",
    "target = db.iloc[:,[9,4,6,7,11,12] + list(range(16,22))].copy()\n",
    "target['Duration'] =db['Ship Date']-db['Order Date']\n",
    "target['Ship Day'] = db['Ship Date'].dt.dayofweek\n",
    "target['Ship Month']= db['Ship Date'].dt.month\n",
    "target['Order Month'] = db['Order Date'].dt.month\n",
    "target['Returned']=db['Returned'].map({'Yes':1,'Not':0})\n",
    "\n",
    "def extract_brand(product_name):\n",
    "    if isinstance(product_name,str):\n",
    "        return product_name.split()[0].strip(\",\").title()\n",
    "    return \"Unknown\"\n",
    "target['Brand']=target['Product Name'].apply(extract_brand)\n",
    "target['Duration']=target['Duration'].dt.days\n",
    "encoding_cols = ['Ship Mode','Segment','City','Sub-Category','Brand','Region']\n",
    "\n",
    "for cols in encoding_cols:\n",
    "    for cols in encoding_cols:\n",
    "        le = LabelEncoder()\n",
    "        target[cols] = le.fit_transform(target[cols])\n",
    "target.drop(columns=['Product Name','Customer Name','Sales'],inplace= True)\n",
    "\n",
    "X = target.drop(columns=['Returned'])\n",
    "y = target['Returned']\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)\n",
    "x_est,x_final,y_est,y_final = train_test_split(x_train,y_train,test_size=0.2,random_state=42,stratify =y_train )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cf69c7-205b-4aab-a11c-cd65b3728694",
   "metadata": {},
   "source": [
    "Optuna Tuning For XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78f6d9d6-62eb-4743-9714-0bf3de997232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-27 22:36:21,362] A new study created in memory with name: no-name-bca55835-2782-4ddd-ac86-c584358b2edc\n",
      "[I 2025-06-27 22:36:21,692] Trial 0 finished with value: 0.919959979989995 and parameters: {'n_estimators': 250, 'max_depth': 25, 'learning_rate': 0.01, 'subsample': 0.5572866586572861, 'colsample_bytree': 0.6816710794916206, 'gamma': 4.47876309545138, 'reg_alpha': 1.850725997796826, 'reg_lambda': 4.760376422921324}. Best is trial 0 with value: 0.919959979989995.\n",
      "[I 2025-06-27 22:36:21,969] Trial 1 finished with value: 0.919959979989995 and parameters: {'n_estimators': 100, 'max_depth': 15, 'learning_rate': 0.01, 'subsample': 0.8288675217618944, 'colsample_bytree': 0.5937501904651324, 'gamma': 0.8532033061017918, 'reg_alpha': 1.7364792372494575, 'reg_lambda': 2.047570811489779}. Best is trial 0 with value: 0.919959979989995.\n",
      "[I 2025-06-27 22:36:22,263] Trial 2 finished with value: 0.9224612306153076 and parameters: {'n_estimators': 250, 'max_depth': 5, 'learning_rate': 0.3, 'subsample': 0.8735719385283629, 'colsample_bytree': 0.9083390944557697, 'gamma': 3.2559685737668214, 'reg_alpha': 4.373852947175868, 'reg_lambda': 0.9519661108223093}. Best is trial 2 with value: 0.9224612306153076.\n",
      "[I 2025-06-27 22:36:22,491] Trial 3 finished with value: 0.9204602301150575 and parameters: {'n_estimators': 100, 'max_depth': 25, 'learning_rate': 0.1, 'subsample': 0.5365197580611889, 'colsample_bytree': 0.6474574447872502, 'gamma': 3.5467295101134595, 'reg_alpha': 3.824909724996804, 'reg_lambda': 0.5458957974774697}. Best is trial 2 with value: 0.9224612306153076.\n",
      "[I 2025-06-27 22:36:22,717] Trial 4 finished with value: 0.92096048024012 and parameters: {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.1, 'subsample': 0.7234656710201908, 'colsample_bytree': 0.6927264196308802, 'gamma': 4.38425091942592, 'reg_alpha': 2.752895408569773, 'reg_lambda': 0.3914288737457333}. Best is trial 2 with value: 0.9224612306153076.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”§ Best XGBoost Parameters:\n",
      "{'n_estimators': 250, 'max_depth': 5, 'learning_rate': 0.3, 'subsample': 0.8735719385283629, 'colsample_bytree': 0.9083390944557697, 'gamma': 3.2559685737668214, 'reg_alpha': 4.373852947175868, 'reg_lambda': 0.9519661108223093}\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_categorical('n_estimators', [100, 250, 350]),\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [5, 15, 25]),\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', [0.01, 0.1, 0.3]),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 5.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 5.0),\n",
    "        'eval_metric': 'logloss',\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = XGBClassifier(**params)\n",
    "    model.fit(x_train, y_train)\n",
    "    preds = model.predict(x_test)\n",
    "    return accuracy_score(y_test, preds)\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=5)\n",
    "print(\"\\nðŸ”§ Best XGBoost Parameters:\")\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07677c08-cd79-402c-8682-aa76c70a58f0",
   "metadata": {},
   "source": [
    "GridSearchCV : LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ffd0efb-f450-436b-9914-4357aa971dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 640, number of negative: 7355\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000317 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7995, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080050 -> initscore=-2.441667\n",
      "[LightGBM] [Info] Start training from score -2.441667\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Best Params:  {'learning_rate': 0.1, 'max_depth': 8, 'min_child_samples': 10, 'n_estimators': 300, 'num_leaves': 63, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "lgbm_param={\n",
    "    'n_estimators': [100, 300],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'num_leaves': [15, 31, 63],\n",
    "    'min_child_samples': [10, 20],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "LGBM_model=LGBMClassifier(random_seed=42)\n",
    "lgbm_grid=GridSearchCV(\n",
    "    estimator=LGBM_model,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    param_grid=lgbm_param\n",
    ")\n",
    "lgbm_grid.fit(x_train,y_train)\n",
    "print('Best Params: ',lgbm_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9fce01-5907-49d7-b9dc-54b95f8e78a8",
   "metadata": {},
   "source": [
    "GridSearchCV : Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7292213-7f3b-428f-8834-6d27523d9e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params={\n",
    "    'n_estimators':[100,300],\n",
    "    'max_depth':[10,20,None],\n",
    "    'min_samples_split':[2,5],\n",
    "    'min_samples_leaf':[1,2],\n",
    "    'bootstrap':[True,False]\n",
    "}\n",
    "rf_search=RandomForestClassifier(random_state=42)\n",
    "rf_best=GridSearchCV(\n",
    "    estimator= rf_search,\n",
    "    param_grid= rf_params,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_best.fit(x_train,y_train)\n",
    "print(\"Random Forest: \",rf_best.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b0ce68-7388-4329-adfc-c904d7a09d25",
   "metadata": {},
   "source": [
    "GridSearchCV : CATBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202da1ea-b809-40fb-9770-86fea4de4c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_param_grid = {\n",
    "    'iterations':[100,300],\n",
    "    'depth':[4,6,8],\n",
    "    'learning_rate':[0.01,0.1],\n",
    "    'border_count':[32,64],\n",
    "    'verbose':[0]\n",
    "}\n",
    "CAT_model = CatBoostClassifier(random_seed=42)\n",
    "CAT_grid=GridSearchCV(\n",
    "    estimator=CAT_model,\n",
    "    param_grid=cat_param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "CAT_grid.fit(x_train,y_train)\n",
    "print(\"Best Case: \",CAT_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a169c4-e542-4d71-809a-0ea3200df6b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
